{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "detectors = {\n",
    "    \"HHEMv1\":\"HHEM-1\", \n",
    "    \"HHEM-2.1\": \"HHEM-2.1-Tri\" , \n",
    "    \"HHEM-2.1-English\": \"HHEM-2.1-English\", \n",
    "    \"HHEM-2.1-Open\": \"HHEM-2.1-Open\",\n",
    "    \"alignscore-base\": \"AlignScore-BS\",\n",
    "    \"alignscore-large\": \"AlignScore-LG\",\n",
    "    \"trueteacher\": \"True-Teacher\", \n",
    "    \"true_nli\": \"True-NLI\", \n",
    "    \"gpt-3.5-turbo\": \"GPT-3.5-Turbo, zero-shot\", \n",
    "    \"gpt-4-turbo\": \"GPT-4-Turbo, zero-shot\", \n",
    "    \"gpt-4o\": \"GPT-4o, zero-shot\", \n",
    "    \"gpt-4\": \"GPT-4, zero-shot\",\n",
    "    \"minicheck-roberta-large\": \"Minicheck-Roberta-LG\",\n",
    "    \"minicheck-deberta-v3-large\": \"Minicheck-Deberta-LG\",\n",
    "    \"minicheck-flan-t5-large\": \"Minicheck-Flan-T5-LG\",\n",
    "    \"Ragas_gpt-4o\": \"Ragas-GPT-4o\",\n",
    "    \"Trulens_gpt-4o_scores\": \"Trulens-GPT-4o\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_pooling(labels):\n",
    "    if 'Consistent' in labels:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def worst_pooling(labels):\n",
    "    if 'Unwanted' in labels or 'Questionable' in labels or 'Benign' in labels:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_predictions(method):\n",
    "    '''\n",
    "    method: `best pooling` or  `worst pooling`\n",
    "    '''\n",
    "    assert method in ['worst-pooling', 'best-pooling'], 'Only support \\'worst-pooling\\' or \\'best-pooling\\''\n",
    "    predictions = {detector: [] for detector in ['human'] + list(detectors.keys())}\n",
    "    for detector in predictions:\n",
    "        # print(detector)\n",
    "        if 'ragas' in detector.lower():\n",
    "            with open('processed_ragas_claim_level_preds.jsonl') as reader:\n",
    "                for record in jsonlines.Reader(reader):\n",
    "                    for sent, sent_result in record['results'].items():\n",
    "                        if len(sent_result['claims']) < 1: # no prediction\n",
    "                            predictions[detector].append(1)\n",
    "                        elif 0 in sent_result['claim_preds']:\n",
    "                            predictions[detector].append(0)\n",
    "                        else:\n",
    "                            predictions[detector].append(1)\n",
    "        elif 'trulens' in detector.lower():\n",
    "            with open('processed_trulens_claim_level_preds.jsonl') as reader:\n",
    "                for record in jsonlines.Reader(reader):\n",
    "                        for sent, sent_result in record['results'].items():\n",
    "                            if len(sent_result['claims']) < 1: # no prediction\n",
    "                                predictions[detector].append(1)\n",
    "                            elif np.mean(sent_result['claim_preds']) < 1:\n",
    "                                predictions[detector].append(0)\n",
    "                            else:\n",
    "                                predictions[detector].append(1)\n",
    "        else:\n",
    "            with open('dectectors_claim_level_preds.json') as f:\n",
    "                data = json.load(f)\n",
    "                for meta_id in data:\n",
    "                    record = data[meta_id]\n",
    "                    for _, results in record.items():\n",
    "                        if 'human' in detector.lower():\n",
    "                            if method == 'best-pooling':\n",
    "                                predictions[detector].append(best_pooling(results['labels']))\n",
    "                            else:\n",
    "                                predictions[detector].append(worst_pooling(results['labels']))\n",
    "                        else:\n",
    "                            if results[detector] is not None:\n",
    "                                predictions[detector].append(int(results[detector] > 0.5))\n",
    "                            else: # prediction is None. may occur for trueteacher/truenli\n",
    "                                if method == 'best-pooling':\n",
    "                                    predictions[detector].append(1-best_pooling(results['labels']))\n",
    "                                else:\n",
    "                                    predictions[detector].append(1-worst_pooling(results['labels']))\n",
    "                                \n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_performance(pred_df):\n",
    "    performance_results = {}\n",
    "    for detector in list(detectors.keys()):\n",
    "        detector_results = {\n",
    "            \"ba\": round(balanced_accuracy_score(pred_df['human'], pred_df[detector])*100,2),\n",
    "            \"f1-macro\": round(f1_score(pred_df['human'], pred_df[detector], pos_label=1, average=\"macro\")*100,2),\n",
    "            # \"f1-halu\": round(f1_score(self.pred_df['human'], self.pred_df[detector], pos_label=0)*100,2),\n",
    "            # \"pr-halu\": round(precision_score(self.pred_df['human'], self.pred_df[detector], pos_label=0)*100,2),\n",
    "            # 're-halu': round(recall_score(self.pred_df['human'], self.pred_df[detector], pos_label=0)*100,2),\n",
    "            # \"f1-cons\": round(f1_score(self.pred_df['human'], self.pred_df[detector], pos_label=1)*100,2),\n",
    "            # \"pr-cons\": round(precision_score(self.pred_df['human'], self.pred_df[detector], pos_label=1)*100,2),\n",
    "            # 're-cons': round(recall_score(self.pred_df['human'], self.pred_df[detector], pos_label=1)*100,2)\n",
    "        }\n",
    "        performance_results[detector] = detector_results\n",
    "        df = pd.DataFrame.from_dict(performance_results, orient='index')\n",
    "        df = df.rename(columns=detectors)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ba</th>\n",
       "      <th>f1-macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>HHEMv1</th>\n",
       "      <td>49.96</td>\n",
       "      <td>49.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HHEM-2.1</th>\n",
       "      <td>54.15</td>\n",
       "      <td>50.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HHEM-2.1-English</th>\n",
       "      <td>52.72</td>\n",
       "      <td>47.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HHEM-2.1-Open</th>\n",
       "      <td>54.36</td>\n",
       "      <td>50.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alignscore-base</th>\n",
       "      <td>53.30</td>\n",
       "      <td>52.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alignscore-large</th>\n",
       "      <td>55.96</td>\n",
       "      <td>55.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trueteacher</th>\n",
       "      <td>51.38</td>\n",
       "      <td>48.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true_nli</th>\n",
       "      <td>50.89</td>\n",
       "      <td>48.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-3.5-turbo</th>\n",
       "      <td>50.00</td>\n",
       "      <td>39.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4-turbo</th>\n",
       "      <td>50.00</td>\n",
       "      <td>39.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4o</th>\n",
       "      <td>50.00</td>\n",
       "      <td>39.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4</th>\n",
       "      <td>50.00</td>\n",
       "      <td>39.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minicheck-roberta-large</th>\n",
       "      <td>56.68</td>\n",
       "      <td>56.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minicheck-deberta-v3-large</th>\n",
       "      <td>58.39</td>\n",
       "      <td>58.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minicheck-flan-t5-large</th>\n",
       "      <td>55.90</td>\n",
       "      <td>55.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ragas_gpt-4o</th>\n",
       "      <td>49.96</td>\n",
       "      <td>46.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trulens_gpt-4o_scores</th>\n",
       "      <td>50.08</td>\n",
       "      <td>44.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               ba  f1-macro\n",
       "HHEMv1                      49.96     49.02\n",
       "HHEM-2.1                    54.15     50.36\n",
       "HHEM-2.1-English            52.72     47.36\n",
       "HHEM-2.1-Open               54.36     50.78\n",
       "alignscore-base             53.30     52.77\n",
       "alignscore-large            55.96     55.84\n",
       "trueteacher                 51.38     48.39\n",
       "true_nli                    50.89     48.62\n",
       "gpt-3.5-turbo               50.00     39.77\n",
       "gpt-4-turbo                 50.00     39.77\n",
       "gpt-4o                      50.00     39.77\n",
       "gpt-4                       50.00     39.77\n",
       "minicheck-roberta-large     56.68     56.67\n",
       "minicheck-deberta-v3-large  58.39     58.49\n",
       "minicheck-flan-t5-large     55.90     55.77\n",
       "Ragas_gpt-4o                49.96     46.25\n",
       "Trulens_gpt-4o_scores       50.08     44.24"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = load_predictions('best-pooling')\n",
    "# print(preds)\n",
    "# for detector in preds:\n",
    "#     print(detector)\n",
    "#     print(len(preds[detector]))\n",
    "pred_df = pd.DataFrame(preds)\n",
    "best_pooling_results = compute_performance(pred_df)\n",
    "best_pooling_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ba</th>\n",
       "      <th>f1-macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>HHEMv1</th>\n",
       "      <td>49.96</td>\n",
       "      <td>49.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HHEM-2.1</th>\n",
       "      <td>54.15</td>\n",
       "      <td>50.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HHEM-2.1-English</th>\n",
       "      <td>52.72</td>\n",
       "      <td>47.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HHEM-2.1-Open</th>\n",
       "      <td>54.36</td>\n",
       "      <td>50.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alignscore-base</th>\n",
       "      <td>53.30</td>\n",
       "      <td>52.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alignscore-large</th>\n",
       "      <td>55.96</td>\n",
       "      <td>55.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trueteacher</th>\n",
       "      <td>51.38</td>\n",
       "      <td>48.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true_nli</th>\n",
       "      <td>50.89</td>\n",
       "      <td>48.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-3.5-turbo</th>\n",
       "      <td>50.00</td>\n",
       "      <td>39.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4-turbo</th>\n",
       "      <td>50.00</td>\n",
       "      <td>39.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4o</th>\n",
       "      <td>50.00</td>\n",
       "      <td>39.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4</th>\n",
       "      <td>50.00</td>\n",
       "      <td>39.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minicheck-roberta-large</th>\n",
       "      <td>56.68</td>\n",
       "      <td>56.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minicheck-deberta-v3-large</th>\n",
       "      <td>58.39</td>\n",
       "      <td>58.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minicheck-flan-t5-large</th>\n",
       "      <td>55.90</td>\n",
       "      <td>55.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ragas_gpt-4o</th>\n",
       "      <td>49.96</td>\n",
       "      <td>46.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trulens_gpt-4o_scores</th>\n",
       "      <td>50.08</td>\n",
       "      <td>44.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               ba  f1-macro\n",
       "HHEMv1                      49.96     49.02\n",
       "HHEM-2.1                    54.15     50.36\n",
       "HHEM-2.1-English            52.72     47.36\n",
       "HHEM-2.1-Open               54.36     50.78\n",
       "alignscore-base             53.30     52.77\n",
       "alignscore-large            55.96     55.84\n",
       "trueteacher                 51.38     48.39\n",
       "true_nli                    50.89     48.62\n",
       "gpt-3.5-turbo               50.00     39.77\n",
       "gpt-4-turbo                 50.00     39.77\n",
       "gpt-4o                      50.00     39.77\n",
       "gpt-4                       50.00     39.77\n",
       "minicheck-roberta-large     56.68     56.67\n",
       "minicheck-deberta-v3-large  58.39     58.49\n",
       "minicheck-flan-t5-large     55.90     55.77\n",
       "Ragas_gpt-4o                49.96     46.25\n",
       "Trulens_gpt-4o_scores       50.08     44.24"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = load_predictions('worst-pooling')\n",
    "pred_df = pd.DataFrame(preds)\n",
    "worst_pooling_results = compute_performance(pred_df)\n",
    "worst_pooling_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
