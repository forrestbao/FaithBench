{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from batch ID to the corresponding sample IDs\n",
    "skip_samples = {\n",
    "    5: range(40, 50), \n",
    "    10: range(10,20),\n",
    "    12: range(20, 30), \n",
    "    15: range(40, 50)\n",
    "}\n",
    "\n",
    "annotator_list = {\n",
    "    7: ['yujia', 'manveer'],\n",
    "    8: ['miaoran', 'chenyu'],\n",
    "    # 11: ['rogger', 'matt'] #'yujia',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(760, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miaoran/FaithBench/assign/utils.py:192: RuntimeWarning: invalid value encountered in cast\n",
      "  detector_pred = row[f\"{detector}\"].astype(int)\n"
     ]
    }
   ],
   "source": [
    "result_path = 'batch_5_src_no_sports/results'\n",
    "result_files = []\n",
    "skip_sample_ids = {}\n",
    "selected_annotators = {}\n",
    "for batch_id in range(1,16+1):\n",
    "    file_name = os.path.join(result_path, f\"batch_{batch_id}_annotation.json\")\n",
    "    result_files.append(file_name)\n",
    "    if batch_id in skip_samples:\n",
    "        skip_sample_ids[file_name] = skip_samples[batch_id]\n",
    "    if batch_id in annotator_list:\n",
    "        selected_annotators[file_name] = annotator_list[batch_id]\n",
    "# result_files = [os.path.join(result_path, f\"batch_{batch_id}_annotation.json\") for batch_id in range(1,10)]\n",
    "# skip_sample_ids = {os.path.join(result_path, \"batch_5_annotation.json\"): list(range(40,50))}\n",
    "# selected_annotators = {\n",
    "#         # os.path.join(result_path, \"batch_3_annotation.json\"): ['yujia', 'rogger'],\n",
    "#         os.path.join(result_path, \"batch_7_annotation.json\"): ['yujia', 'manveer']\n",
    "# }\n",
    "detector_eval = DetectorEvaluator(result_files, skip_sample_ids=skip_sample_ids, selected_annotators=selected_annotators)\n",
    "detector_eval.process_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human</th>\n",
       "      <th>HHEMv1</th>\n",
       "      <th>HHEM-2.1</th>\n",
       "      <th>HHEM-2.1-English</th>\n",
       "      <th>HHEM-2.1-Open</th>\n",
       "      <th>trueteacher</th>\n",
       "      <th>true_nli</th>\n",
       "      <th>gpt-3.5-turbo</th>\n",
       "      <th>gpt-4-turbo</th>\n",
       "      <th>gpt-4o</th>\n",
       "      <th>gpt-4</th>\n",
       "      <th>minicheck-roberta-large</th>\n",
       "      <th>minicheck-deberta-v3-large</th>\n",
       "      <th>minicheck-flan-t5-large</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>human</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.008834</td>\n",
       "      <td>0.147277</td>\n",
       "      <td>0.106371</td>\n",
       "      <td>0.051839</td>\n",
       "      <td>0.105509</td>\n",
       "      <td>0.042319</td>\n",
       "      <td>-0.137193</td>\n",
       "      <td>0.160166</td>\n",
       "      <td>0.153809</td>\n",
       "      <td>0.110584</td>\n",
       "      <td>0.098444</td>\n",
       "      <td>0.095696</td>\n",
       "      <td>-0.010140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HHEMv1</th>\n",
       "      <td>0.008834</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.023409</td>\n",
       "      <td>-0.018517</td>\n",
       "      <td>-0.021494</td>\n",
       "      <td>-0.071840</td>\n",
       "      <td>-0.015995</td>\n",
       "      <td>-0.196584</td>\n",
       "      <td>-0.138916</td>\n",
       "      <td>-0.088961</td>\n",
       "      <td>-0.029710</td>\n",
       "      <td>0.134797</td>\n",
       "      <td>0.113995</td>\n",
       "      <td>0.111943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HHEM-2.1</th>\n",
       "      <td>0.147277</td>\n",
       "      <td>-0.023409</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.489007</td>\n",
       "      <td>0.293617</td>\n",
       "      <td>0.218769</td>\n",
       "      <td>0.136771</td>\n",
       "      <td>0.005501</td>\n",
       "      <td>0.079574</td>\n",
       "      <td>0.081216</td>\n",
       "      <td>0.202377</td>\n",
       "      <td>0.082082</td>\n",
       "      <td>0.109582</td>\n",
       "      <td>0.106265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HHEM-2.1-English</th>\n",
       "      <td>0.106371</td>\n",
       "      <td>-0.018517</td>\n",
       "      <td>0.489007</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.266983</td>\n",
       "      <td>0.170236</td>\n",
       "      <td>0.113970</td>\n",
       "      <td>-0.043079</td>\n",
       "      <td>0.097116</td>\n",
       "      <td>0.134721</td>\n",
       "      <td>0.140046</td>\n",
       "      <td>0.033944</td>\n",
       "      <td>0.104192</td>\n",
       "      <td>0.187267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HHEM-2.1-Open</th>\n",
       "      <td>0.051839</td>\n",
       "      <td>-0.021494</td>\n",
       "      <td>0.293617</td>\n",
       "      <td>0.266983</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.110838</td>\n",
       "      <td>0.040225</td>\n",
       "      <td>-0.069685</td>\n",
       "      <td>0.013700</td>\n",
       "      <td>0.051658</td>\n",
       "      <td>0.046980</td>\n",
       "      <td>0.045952</td>\n",
       "      <td>0.110565</td>\n",
       "      <td>0.157900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trueteacher</th>\n",
       "      <td>0.105509</td>\n",
       "      <td>-0.071840</td>\n",
       "      <td>0.218769</td>\n",
       "      <td>0.170236</td>\n",
       "      <td>0.110838</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.211395</td>\n",
       "      <td>0.135162</td>\n",
       "      <td>0.205069</td>\n",
       "      <td>0.185088</td>\n",
       "      <td>0.209380</td>\n",
       "      <td>0.092886</td>\n",
       "      <td>0.042752</td>\n",
       "      <td>0.054390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true_nli</th>\n",
       "      <td>0.042319</td>\n",
       "      <td>-0.015995</td>\n",
       "      <td>0.136771</td>\n",
       "      <td>0.113970</td>\n",
       "      <td>0.040225</td>\n",
       "      <td>0.211395</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.094910</td>\n",
       "      <td>0.161985</td>\n",
       "      <td>0.156784</td>\n",
       "      <td>0.173259</td>\n",
       "      <td>0.076862</td>\n",
       "      <td>0.041792</td>\n",
       "      <td>0.005255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-3.5-turbo</th>\n",
       "      <td>-0.137193</td>\n",
       "      <td>-0.196584</td>\n",
       "      <td>0.005501</td>\n",
       "      <td>-0.043079</td>\n",
       "      <td>-0.069685</td>\n",
       "      <td>0.135162</td>\n",
       "      <td>0.094910</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.164184</td>\n",
       "      <td>0.146747</td>\n",
       "      <td>0.146023</td>\n",
       "      <td>0.029935</td>\n",
       "      <td>0.009963</td>\n",
       "      <td>0.191457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4-turbo</th>\n",
       "      <td>0.160166</td>\n",
       "      <td>-0.138916</td>\n",
       "      <td>0.079574</td>\n",
       "      <td>0.097116</td>\n",
       "      <td>0.013700</td>\n",
       "      <td>0.205069</td>\n",
       "      <td>0.161985</td>\n",
       "      <td>0.164184</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.526851</td>\n",
       "      <td>0.422631</td>\n",
       "      <td>0.078662</td>\n",
       "      <td>0.008006</td>\n",
       "      <td>0.060505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4o</th>\n",
       "      <td>0.153809</td>\n",
       "      <td>-0.088961</td>\n",
       "      <td>0.081216</td>\n",
       "      <td>0.134721</td>\n",
       "      <td>0.051658</td>\n",
       "      <td>0.185088</td>\n",
       "      <td>0.156784</td>\n",
       "      <td>0.146747</td>\n",
       "      <td>0.526851</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.490233</td>\n",
       "      <td>-0.000552</td>\n",
       "      <td>0.021110</td>\n",
       "      <td>0.047609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4</th>\n",
       "      <td>0.110584</td>\n",
       "      <td>-0.029710</td>\n",
       "      <td>0.202377</td>\n",
       "      <td>0.140046</td>\n",
       "      <td>0.046980</td>\n",
       "      <td>0.209380</td>\n",
       "      <td>0.173259</td>\n",
       "      <td>0.146023</td>\n",
       "      <td>0.422631</td>\n",
       "      <td>0.490233</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.089911</td>\n",
       "      <td>0.054581</td>\n",
       "      <td>0.111360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minicheck-roberta-large</th>\n",
       "      <td>0.098444</td>\n",
       "      <td>0.134797</td>\n",
       "      <td>0.082082</td>\n",
       "      <td>0.033944</td>\n",
       "      <td>0.045952</td>\n",
       "      <td>0.092886</td>\n",
       "      <td>0.076862</td>\n",
       "      <td>0.029935</td>\n",
       "      <td>0.078662</td>\n",
       "      <td>-0.000552</td>\n",
       "      <td>0.089911</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.113437</td>\n",
       "      <td>0.248183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minicheck-deberta-v3-large</th>\n",
       "      <td>0.095696</td>\n",
       "      <td>0.113995</td>\n",
       "      <td>0.109582</td>\n",
       "      <td>0.104192</td>\n",
       "      <td>0.110565</td>\n",
       "      <td>0.042752</td>\n",
       "      <td>0.041792</td>\n",
       "      <td>0.009963</td>\n",
       "      <td>0.008006</td>\n",
       "      <td>0.021110</td>\n",
       "      <td>0.054581</td>\n",
       "      <td>0.113437</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.215999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minicheck-flan-t5-large</th>\n",
       "      <td>-0.010140</td>\n",
       "      <td>0.111943</td>\n",
       "      <td>0.106265</td>\n",
       "      <td>0.187267</td>\n",
       "      <td>0.157900</td>\n",
       "      <td>0.054390</td>\n",
       "      <td>0.005255</td>\n",
       "      <td>0.191457</td>\n",
       "      <td>0.060505</td>\n",
       "      <td>0.047609</td>\n",
       "      <td>0.111360</td>\n",
       "      <td>0.248183</td>\n",
       "      <td>0.215999</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               human    HHEMv1  HHEM-2.1  HHEM-2.1-English  \\\n",
       "human                       1.000000  0.008834  0.147277          0.106371   \n",
       "HHEMv1                      0.008834  1.000000 -0.023409         -0.018517   \n",
       "HHEM-2.1                    0.147277 -0.023409  1.000000          0.489007   \n",
       "HHEM-2.1-English            0.106371 -0.018517  0.489007          1.000000   \n",
       "HHEM-2.1-Open               0.051839 -0.021494  0.293617          0.266983   \n",
       "trueteacher                 0.105509 -0.071840  0.218769          0.170236   \n",
       "true_nli                    0.042319 -0.015995  0.136771          0.113970   \n",
       "gpt-3.5-turbo              -0.137193 -0.196584  0.005501         -0.043079   \n",
       "gpt-4-turbo                 0.160166 -0.138916  0.079574          0.097116   \n",
       "gpt-4o                      0.153809 -0.088961  0.081216          0.134721   \n",
       "gpt-4                       0.110584 -0.029710  0.202377          0.140046   \n",
       "minicheck-roberta-large     0.098444  0.134797  0.082082          0.033944   \n",
       "minicheck-deberta-v3-large  0.095696  0.113995  0.109582          0.104192   \n",
       "minicheck-flan-t5-large    -0.010140  0.111943  0.106265          0.187267   \n",
       "\n",
       "                            HHEM-2.1-Open  trueteacher  true_nli  \\\n",
       "human                            0.051839     0.105509  0.042319   \n",
       "HHEMv1                          -0.021494    -0.071840 -0.015995   \n",
       "HHEM-2.1                         0.293617     0.218769  0.136771   \n",
       "HHEM-2.1-English                 0.266983     0.170236  0.113970   \n",
       "HHEM-2.1-Open                    1.000000     0.110838  0.040225   \n",
       "trueteacher                      0.110838     1.000000  0.211395   \n",
       "true_nli                         0.040225     0.211395  1.000000   \n",
       "gpt-3.5-turbo                   -0.069685     0.135162  0.094910   \n",
       "gpt-4-turbo                      0.013700     0.205069  0.161985   \n",
       "gpt-4o                           0.051658     0.185088  0.156784   \n",
       "gpt-4                            0.046980     0.209380  0.173259   \n",
       "minicheck-roberta-large          0.045952     0.092886  0.076862   \n",
       "minicheck-deberta-v3-large       0.110565     0.042752  0.041792   \n",
       "minicheck-flan-t5-large          0.157900     0.054390  0.005255   \n",
       "\n",
       "                            gpt-3.5-turbo  gpt-4-turbo    gpt-4o     gpt-4  \\\n",
       "human                           -0.137193     0.160166  0.153809  0.110584   \n",
       "HHEMv1                          -0.196584    -0.138916 -0.088961 -0.029710   \n",
       "HHEM-2.1                         0.005501     0.079574  0.081216  0.202377   \n",
       "HHEM-2.1-English                -0.043079     0.097116  0.134721  0.140046   \n",
       "HHEM-2.1-Open                   -0.069685     0.013700  0.051658  0.046980   \n",
       "trueteacher                      0.135162     0.205069  0.185088  0.209380   \n",
       "true_nli                         0.094910     0.161985  0.156784  0.173259   \n",
       "gpt-3.5-turbo                    1.000000     0.164184  0.146747  0.146023   \n",
       "gpt-4-turbo                      0.164184     1.000000  0.526851  0.422631   \n",
       "gpt-4o                           0.146747     0.526851  1.000000  0.490233   \n",
       "gpt-4                            0.146023     0.422631  0.490233  1.000000   \n",
       "minicheck-roberta-large          0.029935     0.078662 -0.000552  0.089911   \n",
       "minicheck-deberta-v3-large       0.009963     0.008006  0.021110  0.054581   \n",
       "minicheck-flan-t5-large          0.191457     0.060505  0.047609  0.111360   \n",
       "\n",
       "                            minicheck-roberta-large  \\\n",
       "human                                      0.098444   \n",
       "HHEMv1                                     0.134797   \n",
       "HHEM-2.1                                   0.082082   \n",
       "HHEM-2.1-English                           0.033944   \n",
       "HHEM-2.1-Open                              0.045952   \n",
       "trueteacher                                0.092886   \n",
       "true_nli                                   0.076862   \n",
       "gpt-3.5-turbo                              0.029935   \n",
       "gpt-4-turbo                                0.078662   \n",
       "gpt-4o                                    -0.000552   \n",
       "gpt-4                                      0.089911   \n",
       "minicheck-roberta-large                    1.000000   \n",
       "minicheck-deberta-v3-large                 0.113437   \n",
       "minicheck-flan-t5-large                    0.248183   \n",
       "\n",
       "                            minicheck-deberta-v3-large  \\\n",
       "human                                         0.095696   \n",
       "HHEMv1                                        0.113995   \n",
       "HHEM-2.1                                      0.109582   \n",
       "HHEM-2.1-English                              0.104192   \n",
       "HHEM-2.1-Open                                 0.110565   \n",
       "trueteacher                                   0.042752   \n",
       "true_nli                                      0.041792   \n",
       "gpt-3.5-turbo                                 0.009963   \n",
       "gpt-4-turbo                                   0.008006   \n",
       "gpt-4o                                        0.021110   \n",
       "gpt-4                                         0.054581   \n",
       "minicheck-roberta-large                       0.113437   \n",
       "minicheck-deberta-v3-large                    1.000000   \n",
       "minicheck-flan-t5-large                       0.215999   \n",
       "\n",
       "                            minicheck-flan-t5-large  \n",
       "human                                     -0.010140  \n",
       "HHEMv1                                     0.111943  \n",
       "HHEM-2.1                                   0.106265  \n",
       "HHEM-2.1-English                           0.187267  \n",
       "HHEM-2.1-Open                              0.157900  \n",
       "trueteacher                                0.054390  \n",
       "true_nli                                   0.005255  \n",
       "gpt-3.5-turbo                              0.191457  \n",
       "gpt-4-turbo                                0.060505  \n",
       "gpt-4o                                     0.047609  \n",
       "gpt-4                                      0.111360  \n",
       "minicheck-roberta-large                    0.248183  \n",
       "minicheck-deberta-v3-large                 0.215999  \n",
       "minicheck-flan-t5-large                    1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detector_eval.compute_correlation('pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human</th>\n",
       "      <th>HHEMv1</th>\n",
       "      <th>HHEM-2.1</th>\n",
       "      <th>HHEM-2.1-English</th>\n",
       "      <th>HHEM-2.1-Open</th>\n",
       "      <th>trueteacher</th>\n",
       "      <th>true_nli</th>\n",
       "      <th>gpt-3.5-turbo</th>\n",
       "      <th>gpt-4-turbo</th>\n",
       "      <th>gpt-4o</th>\n",
       "      <th>gpt-4</th>\n",
       "      <th>minicheck-roberta-large</th>\n",
       "      <th>minicheck-deberta-v3-large</th>\n",
       "      <th>minicheck-flan-t5-large</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>human</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.008834</td>\n",
       "      <td>0.147277</td>\n",
       "      <td>0.106371</td>\n",
       "      <td>0.051839</td>\n",
       "      <td>0.105509</td>\n",
       "      <td>0.042319</td>\n",
       "      <td>-0.137193</td>\n",
       "      <td>0.160166</td>\n",
       "      <td>0.153809</td>\n",
       "      <td>0.110584</td>\n",
       "      <td>0.098444</td>\n",
       "      <td>0.095696</td>\n",
       "      <td>-0.010140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HHEMv1</th>\n",
       "      <td>0.008834</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.023409</td>\n",
       "      <td>-0.018517</td>\n",
       "      <td>-0.021494</td>\n",
       "      <td>-0.071840</td>\n",
       "      <td>-0.015995</td>\n",
       "      <td>-0.196584</td>\n",
       "      <td>-0.138916</td>\n",
       "      <td>-0.088961</td>\n",
       "      <td>-0.029710</td>\n",
       "      <td>0.134797</td>\n",
       "      <td>0.113995</td>\n",
       "      <td>0.111943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HHEM-2.1</th>\n",
       "      <td>0.147277</td>\n",
       "      <td>-0.023409</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.489007</td>\n",
       "      <td>0.293617</td>\n",
       "      <td>0.218769</td>\n",
       "      <td>0.136771</td>\n",
       "      <td>0.005501</td>\n",
       "      <td>0.079574</td>\n",
       "      <td>0.081216</td>\n",
       "      <td>0.202377</td>\n",
       "      <td>0.082082</td>\n",
       "      <td>0.109582</td>\n",
       "      <td>0.106265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HHEM-2.1-English</th>\n",
       "      <td>0.106371</td>\n",
       "      <td>-0.018517</td>\n",
       "      <td>0.489007</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.266983</td>\n",
       "      <td>0.170236</td>\n",
       "      <td>0.113970</td>\n",
       "      <td>-0.043079</td>\n",
       "      <td>0.097116</td>\n",
       "      <td>0.134721</td>\n",
       "      <td>0.140046</td>\n",
       "      <td>0.033944</td>\n",
       "      <td>0.104192</td>\n",
       "      <td>0.187267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HHEM-2.1-Open</th>\n",
       "      <td>0.051839</td>\n",
       "      <td>-0.021494</td>\n",
       "      <td>0.293617</td>\n",
       "      <td>0.266983</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.110838</td>\n",
       "      <td>0.040225</td>\n",
       "      <td>-0.069685</td>\n",
       "      <td>0.013700</td>\n",
       "      <td>0.051658</td>\n",
       "      <td>0.046980</td>\n",
       "      <td>0.045952</td>\n",
       "      <td>0.110565</td>\n",
       "      <td>0.157900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trueteacher</th>\n",
       "      <td>0.105509</td>\n",
       "      <td>-0.071840</td>\n",
       "      <td>0.218769</td>\n",
       "      <td>0.170236</td>\n",
       "      <td>0.110838</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.211395</td>\n",
       "      <td>0.135162</td>\n",
       "      <td>0.205069</td>\n",
       "      <td>0.185088</td>\n",
       "      <td>0.209380</td>\n",
       "      <td>0.092886</td>\n",
       "      <td>0.042752</td>\n",
       "      <td>0.054390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true_nli</th>\n",
       "      <td>0.042319</td>\n",
       "      <td>-0.015995</td>\n",
       "      <td>0.136771</td>\n",
       "      <td>0.113970</td>\n",
       "      <td>0.040225</td>\n",
       "      <td>0.211395</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.094910</td>\n",
       "      <td>0.161985</td>\n",
       "      <td>0.156784</td>\n",
       "      <td>0.173259</td>\n",
       "      <td>0.076862</td>\n",
       "      <td>0.041792</td>\n",
       "      <td>0.005255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-3.5-turbo</th>\n",
       "      <td>-0.137193</td>\n",
       "      <td>-0.196584</td>\n",
       "      <td>0.005501</td>\n",
       "      <td>-0.043079</td>\n",
       "      <td>-0.069685</td>\n",
       "      <td>0.135162</td>\n",
       "      <td>0.094910</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.164184</td>\n",
       "      <td>0.146747</td>\n",
       "      <td>0.146023</td>\n",
       "      <td>0.029935</td>\n",
       "      <td>0.009963</td>\n",
       "      <td>0.191457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4-turbo</th>\n",
       "      <td>0.160166</td>\n",
       "      <td>-0.138916</td>\n",
       "      <td>0.079574</td>\n",
       "      <td>0.097116</td>\n",
       "      <td>0.013700</td>\n",
       "      <td>0.205069</td>\n",
       "      <td>0.161985</td>\n",
       "      <td>0.164184</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.526851</td>\n",
       "      <td>0.422631</td>\n",
       "      <td>0.078662</td>\n",
       "      <td>0.008006</td>\n",
       "      <td>0.060505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4o</th>\n",
       "      <td>0.153809</td>\n",
       "      <td>-0.088961</td>\n",
       "      <td>0.081216</td>\n",
       "      <td>0.134721</td>\n",
       "      <td>0.051658</td>\n",
       "      <td>0.185088</td>\n",
       "      <td>0.156784</td>\n",
       "      <td>0.146747</td>\n",
       "      <td>0.526851</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.490233</td>\n",
       "      <td>-0.000552</td>\n",
       "      <td>0.021110</td>\n",
       "      <td>0.047609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4</th>\n",
       "      <td>0.110584</td>\n",
       "      <td>-0.029710</td>\n",
       "      <td>0.202377</td>\n",
       "      <td>0.140046</td>\n",
       "      <td>0.046980</td>\n",
       "      <td>0.209380</td>\n",
       "      <td>0.173259</td>\n",
       "      <td>0.146023</td>\n",
       "      <td>0.422631</td>\n",
       "      <td>0.490233</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.089911</td>\n",
       "      <td>0.054581</td>\n",
       "      <td>0.111360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minicheck-roberta-large</th>\n",
       "      <td>0.098444</td>\n",
       "      <td>0.134797</td>\n",
       "      <td>0.082082</td>\n",
       "      <td>0.033944</td>\n",
       "      <td>0.045952</td>\n",
       "      <td>0.092886</td>\n",
       "      <td>0.076862</td>\n",
       "      <td>0.029935</td>\n",
       "      <td>0.078662</td>\n",
       "      <td>-0.000552</td>\n",
       "      <td>0.089911</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.113437</td>\n",
       "      <td>0.248183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minicheck-deberta-v3-large</th>\n",
       "      <td>0.095696</td>\n",
       "      <td>0.113995</td>\n",
       "      <td>0.109582</td>\n",
       "      <td>0.104192</td>\n",
       "      <td>0.110565</td>\n",
       "      <td>0.042752</td>\n",
       "      <td>0.041792</td>\n",
       "      <td>0.009963</td>\n",
       "      <td>0.008006</td>\n",
       "      <td>0.021110</td>\n",
       "      <td>0.054581</td>\n",
       "      <td>0.113437</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.215999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minicheck-flan-t5-large</th>\n",
       "      <td>-0.010140</td>\n",
       "      <td>0.111943</td>\n",
       "      <td>0.106265</td>\n",
       "      <td>0.187267</td>\n",
       "      <td>0.157900</td>\n",
       "      <td>0.054390</td>\n",
       "      <td>0.005255</td>\n",
       "      <td>0.191457</td>\n",
       "      <td>0.060505</td>\n",
       "      <td>0.047609</td>\n",
       "      <td>0.111360</td>\n",
       "      <td>0.248183</td>\n",
       "      <td>0.215999</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               human    HHEMv1  HHEM-2.1  HHEM-2.1-English  \\\n",
       "human                       1.000000  0.008834  0.147277          0.106371   \n",
       "HHEMv1                      0.008834  1.000000 -0.023409         -0.018517   \n",
       "HHEM-2.1                    0.147277 -0.023409  1.000000          0.489007   \n",
       "HHEM-2.1-English            0.106371 -0.018517  0.489007          1.000000   \n",
       "HHEM-2.1-Open               0.051839 -0.021494  0.293617          0.266983   \n",
       "trueteacher                 0.105509 -0.071840  0.218769          0.170236   \n",
       "true_nli                    0.042319 -0.015995  0.136771          0.113970   \n",
       "gpt-3.5-turbo              -0.137193 -0.196584  0.005501         -0.043079   \n",
       "gpt-4-turbo                 0.160166 -0.138916  0.079574          0.097116   \n",
       "gpt-4o                      0.153809 -0.088961  0.081216          0.134721   \n",
       "gpt-4                       0.110584 -0.029710  0.202377          0.140046   \n",
       "minicheck-roberta-large     0.098444  0.134797  0.082082          0.033944   \n",
       "minicheck-deberta-v3-large  0.095696  0.113995  0.109582          0.104192   \n",
       "minicheck-flan-t5-large    -0.010140  0.111943  0.106265          0.187267   \n",
       "\n",
       "                            HHEM-2.1-Open  trueteacher  true_nli  \\\n",
       "human                            0.051839     0.105509  0.042319   \n",
       "HHEMv1                          -0.021494    -0.071840 -0.015995   \n",
       "HHEM-2.1                         0.293617     0.218769  0.136771   \n",
       "HHEM-2.1-English                 0.266983     0.170236  0.113970   \n",
       "HHEM-2.1-Open                    1.000000     0.110838  0.040225   \n",
       "trueteacher                      0.110838     1.000000  0.211395   \n",
       "true_nli                         0.040225     0.211395  1.000000   \n",
       "gpt-3.5-turbo                   -0.069685     0.135162  0.094910   \n",
       "gpt-4-turbo                      0.013700     0.205069  0.161985   \n",
       "gpt-4o                           0.051658     0.185088  0.156784   \n",
       "gpt-4                            0.046980     0.209380  0.173259   \n",
       "minicheck-roberta-large          0.045952     0.092886  0.076862   \n",
       "minicheck-deberta-v3-large       0.110565     0.042752  0.041792   \n",
       "minicheck-flan-t5-large          0.157900     0.054390  0.005255   \n",
       "\n",
       "                            gpt-3.5-turbo  gpt-4-turbo    gpt-4o     gpt-4  \\\n",
       "human                           -0.137193     0.160166  0.153809  0.110584   \n",
       "HHEMv1                          -0.196584    -0.138916 -0.088961 -0.029710   \n",
       "HHEM-2.1                         0.005501     0.079574  0.081216  0.202377   \n",
       "HHEM-2.1-English                -0.043079     0.097116  0.134721  0.140046   \n",
       "HHEM-2.1-Open                   -0.069685     0.013700  0.051658  0.046980   \n",
       "trueteacher                      0.135162     0.205069  0.185088  0.209380   \n",
       "true_nli                         0.094910     0.161985  0.156784  0.173259   \n",
       "gpt-3.5-turbo                    1.000000     0.164184  0.146747  0.146023   \n",
       "gpt-4-turbo                      0.164184     1.000000  0.526851  0.422631   \n",
       "gpt-4o                           0.146747     0.526851  1.000000  0.490233   \n",
       "gpt-4                            0.146023     0.422631  0.490233  1.000000   \n",
       "minicheck-roberta-large          0.029935     0.078662 -0.000552  0.089911   \n",
       "minicheck-deberta-v3-large       0.009963     0.008006  0.021110  0.054581   \n",
       "minicheck-flan-t5-large          0.191457     0.060505  0.047609  0.111360   \n",
       "\n",
       "                            minicheck-roberta-large  \\\n",
       "human                                      0.098444   \n",
       "HHEMv1                                     0.134797   \n",
       "HHEM-2.1                                   0.082082   \n",
       "HHEM-2.1-English                           0.033944   \n",
       "HHEM-2.1-Open                              0.045952   \n",
       "trueteacher                                0.092886   \n",
       "true_nli                                   0.076862   \n",
       "gpt-3.5-turbo                              0.029935   \n",
       "gpt-4-turbo                                0.078662   \n",
       "gpt-4o                                    -0.000552   \n",
       "gpt-4                                      0.089911   \n",
       "minicheck-roberta-large                    1.000000   \n",
       "minicheck-deberta-v3-large                 0.113437   \n",
       "minicheck-flan-t5-large                    0.248183   \n",
       "\n",
       "                            minicheck-deberta-v3-large  \\\n",
       "human                                         0.095696   \n",
       "HHEMv1                                        0.113995   \n",
       "HHEM-2.1                                      0.109582   \n",
       "HHEM-2.1-English                              0.104192   \n",
       "HHEM-2.1-Open                                 0.110565   \n",
       "trueteacher                                   0.042752   \n",
       "true_nli                                      0.041792   \n",
       "gpt-3.5-turbo                                 0.009963   \n",
       "gpt-4-turbo                                   0.008006   \n",
       "gpt-4o                                        0.021110   \n",
       "gpt-4                                         0.054581   \n",
       "minicheck-roberta-large                       0.113437   \n",
       "minicheck-deberta-v3-large                    1.000000   \n",
       "minicheck-flan-t5-large                       0.215999   \n",
       "\n",
       "                            minicheck-flan-t5-large  \n",
       "human                                     -0.010140  \n",
       "HHEMv1                                     0.111943  \n",
       "HHEM-2.1                                   0.106265  \n",
       "HHEM-2.1-English                           0.187267  \n",
       "HHEM-2.1-Open                              0.157900  \n",
       "trueteacher                                0.054390  \n",
       "true_nli                                   0.005255  \n",
       "gpt-3.5-turbo                              0.191457  \n",
       "gpt-4-turbo                                0.060505  \n",
       "gpt-4o                                     0.047609  \n",
       "gpt-4                                      0.111360  \n",
       "minicheck-roberta-large                    0.248183  \n",
       "minicheck-deberta-v3-large                 0.215999  \n",
       "minicheck-flan-t5-large                    1.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detector_eval.compute_correlation('spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ba</th>\n",
       "      <th>f1-macro</th>\n",
       "      <th>f1-halu</th>\n",
       "      <th>pr-halu</th>\n",
       "      <th>re-halu</th>\n",
       "      <th>f1-cons</th>\n",
       "      <th>pr-cons</th>\n",
       "      <th>re-cons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>HHEMv1</th>\n",
       "      <td>50.45</td>\n",
       "      <td>42.73</td>\n",
       "      <td>44.02</td>\n",
       "      <td>71.25</td>\n",
       "      <td>31.84</td>\n",
       "      <td>41.45</td>\n",
       "      <td>29.62</td>\n",
       "      <td>69.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HHEM-2.1</th>\n",
       "      <td>55.56</td>\n",
       "      <td>38.09</td>\n",
       "      <td>28.39</td>\n",
       "      <td>87.50</td>\n",
       "      <td>16.95</td>\n",
       "      <td>47.78</td>\n",
       "      <td>32.01</td>\n",
       "      <td>94.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HHEM-2.1-English</th>\n",
       "      <td>53.29</td>\n",
       "      <td>32.79</td>\n",
       "      <td>18.91</td>\n",
       "      <td>86.36</td>\n",
       "      <td>10.61</td>\n",
       "      <td>46.67</td>\n",
       "      <td>30.84</td>\n",
       "      <td>95.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HHEM-2.1-Open</th>\n",
       "      <td>51.50</td>\n",
       "      <td>30.36</td>\n",
       "      <td>15.15</td>\n",
       "      <td>78.95</td>\n",
       "      <td>8.38</td>\n",
       "      <td>45.57</td>\n",
       "      <td>30.01</td>\n",
       "      <td>94.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trueteacher</th>\n",
       "      <td>53.92</td>\n",
       "      <td>36.36</td>\n",
       "      <td>26.06</td>\n",
       "      <td>83.00</td>\n",
       "      <td>15.46</td>\n",
       "      <td>46.66</td>\n",
       "      <td>31.21</td>\n",
       "      <td>92.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true_nli</th>\n",
       "      <td>50.78</td>\n",
       "      <td>26.01</td>\n",
       "      <td>6.44</td>\n",
       "      <td>81.82</td>\n",
       "      <td>3.35</td>\n",
       "      <td>45.58</td>\n",
       "      <td>29.67</td>\n",
       "      <td>98.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-3.5-turbo</th>\n",
       "      <td>43.71</td>\n",
       "      <td>33.11</td>\n",
       "      <td>28.53</td>\n",
       "      <td>59.06</td>\n",
       "      <td>18.81</td>\n",
       "      <td>37.68</td>\n",
       "      <td>25.98</td>\n",
       "      <td>68.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4-turbo</th>\n",
       "      <td>56.41</td>\n",
       "      <td>40.08</td>\n",
       "      <td>31.96</td>\n",
       "      <td>87.50</td>\n",
       "      <td>19.55</td>\n",
       "      <td>48.20</td>\n",
       "      <td>32.50</td>\n",
       "      <td>93.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4o</th>\n",
       "      <td>55.43</td>\n",
       "      <td>36.72</td>\n",
       "      <td>25.56</td>\n",
       "      <td>89.89</td>\n",
       "      <td>14.90</td>\n",
       "      <td>47.87</td>\n",
       "      <td>31.89</td>\n",
       "      <td>95.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4</th>\n",
       "      <td>52.68</td>\n",
       "      <td>29.56</td>\n",
       "      <td>12.50</td>\n",
       "      <td>92.31</td>\n",
       "      <td>6.70</td>\n",
       "      <td>46.61</td>\n",
       "      <td>30.51</td>\n",
       "      <td>98.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minicheck-roberta-large</th>\n",
       "      <td>55.38</td>\n",
       "      <td>53.49</td>\n",
       "      <td>65.48</td>\n",
       "      <td>74.70</td>\n",
       "      <td>58.29</td>\n",
       "      <td>41.49</td>\n",
       "      <td>34.31</td>\n",
       "      <td>52.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minicheck-deberta-v3-large</th>\n",
       "      <td>54.99</td>\n",
       "      <td>54.63</td>\n",
       "      <td>71.17</td>\n",
       "      <td>73.80</td>\n",
       "      <td>68.72</td>\n",
       "      <td>38.10</td>\n",
       "      <td>35.38</td>\n",
       "      <td>41.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minicheck-flan-t5-large</th>\n",
       "      <td>49.44</td>\n",
       "      <td>47.87</td>\n",
       "      <td>60.21</td>\n",
       "      <td>70.22</td>\n",
       "      <td>52.70</td>\n",
       "      <td>35.52</td>\n",
       "      <td>28.85</td>\n",
       "      <td>46.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               ba  f1-macro  f1-halu  pr-halu  re-halu  \\\n",
       "HHEMv1                      50.45     42.73    44.02    71.25    31.84   \n",
       "HHEM-2.1                    55.56     38.09    28.39    87.50    16.95   \n",
       "HHEM-2.1-English            53.29     32.79    18.91    86.36    10.61   \n",
       "HHEM-2.1-Open               51.50     30.36    15.15    78.95     8.38   \n",
       "trueteacher                 53.92     36.36    26.06    83.00    15.46   \n",
       "true_nli                    50.78     26.01     6.44    81.82     3.35   \n",
       "gpt-3.5-turbo               43.71     33.11    28.53    59.06    18.81   \n",
       "gpt-4-turbo                 56.41     40.08    31.96    87.50    19.55   \n",
       "gpt-4o                      55.43     36.72    25.56    89.89    14.90   \n",
       "gpt-4                       52.68     29.56    12.50    92.31     6.70   \n",
       "minicheck-roberta-large     55.38     53.49    65.48    74.70    58.29   \n",
       "minicheck-deberta-v3-large  54.99     54.63    71.17    73.80    68.72   \n",
       "minicheck-flan-t5-large     49.44     47.87    60.21    70.22    52.70   \n",
       "\n",
       "                            f1-cons  pr-cons  re-cons  \n",
       "HHEMv1                        41.45    29.62    69.06  \n",
       "HHEM-2.1                      47.78    32.01    94.17  \n",
       "HHEM-2.1-English              46.67    30.84    95.96  \n",
       "HHEM-2.1-Open                 45.57    30.01    94.62  \n",
       "trueteacher                   46.66    31.21    92.38  \n",
       "true_nli                      45.58    29.67    98.21  \n",
       "gpt-3.5-turbo                 37.68    25.98    68.61  \n",
       "gpt-4-turbo                   48.20    32.50    93.27  \n",
       "gpt-4o                        47.87    31.89    95.96  \n",
       "gpt-4                         46.61    30.51    98.65  \n",
       "minicheck-roberta-large       41.49    34.31    52.47  \n",
       "minicheck-deberta-v3-large    38.10    35.38    41.26  \n",
       "minicheck-flan-t5-large       35.52    28.85    46.19  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detector_eval.compute_performance()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
