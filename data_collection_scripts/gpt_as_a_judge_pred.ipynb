{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "# caution: path[0] is reserved for script path (or '' in REPL)\n",
    "sys.path.insert(1, '../eval')\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "label_mapping = {'yes':1, 'no':0}\n",
    "gpt_models = ['gpt-3.5-turbo', 'gpt-4-turbo', 'gpt-4o', 'gpt-4']\n",
    "\n",
    "# Aggrefact prompt\n",
    "system = 'Decide if the following summary is consistent with the correponding article. Note that consistency means all information in the summary is supported by the article.'\n",
    "user = 'Article: {article}\\nSummary: {summary}\\nAnswer (yes or no):'\n",
    "\n",
    "def call_gpt(system_prompt, user_prompt, model='gpt-4', temperature=0):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=temperature\n",
    "    )\n",
    "    result = completion.choices[0].message.content\n",
    "    result = re.sub(r'[^\\w\\s]', '', result)\n",
    "    result = label_mapping[result.strip().lower().split()[0]]\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample-level Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'backup_data_with_detector_results')\n",
    "subfolders = [f.path for f in os.scandir(data_folder) if f.is_dir()]\n",
    "model_files = []\n",
    "for subfolder in subfolders:\n",
    "    model_files += [f.path for f in os.scandir(subfolder) if f.is_file()]\n",
    "print(model_files)\n",
    "selected_models = [\n",
    "    \"openai/GPT-3.5-Turbo\",\n",
    "    \"openai/gpt-4o\",\n",
    "    \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    \"cohere/command-r-08-2024\",\n",
    "    \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    \"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
    "    \"google/gemini-1.5-flash-001\",\n",
    "    \"Anthropic/claude-3-5-sonnet-20240620\",\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "]\n",
    "\n",
    "for idx, gpt_model in enumerate(gpt_models):\n",
    "    print(f\"Run model {idx} - {gpt_model}\")\n",
    "    for file_name in model_files:\n",
    "        print(file_name.replace(data_folder,'')[1:].replace('.csv',''))\n",
    "        if file_name.replace(data_folder,'')[1:].replace('.csv','') not in selected_models:\n",
    "            continue\n",
    "        with open(file_name) as f:\n",
    "            print(file_name)\n",
    "            df = pd.read_csv(file_name).fillna('')\n",
    "            preds = []\n",
    "            for index, row in df.iterrows():\n",
    "                # start_time = time.time()\n",
    "                result = call_gpt(system, user.format(article=row['source'], summary=row['summary']), model=gpt_model)\n",
    "                preds.append(result)\n",
    "                    \n",
    "            df.insert(len(df.columns.tolist()), gpt_model, preds)\n",
    "            df.to_csv(file_name, mode='w', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sent-level Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_level_labels = {}\n",
    "result_files, skip_sample_ids, selected_annotators, num_annotators = process_result_files()\n",
    "for file_path in result_files:\n",
    "    _, _, _, batch_sent_level_labels = read_annotation(file_path, skip_sample_ids=skip_sample_ids)\n",
    "    # print(batch_sent_level_labels)\n",
    "    sent_level_labels.update(batch_sent_level_labels)\n",
    "\n",
    "\n",
    "fname = '../eval/sent_level_results/detectors_sent_level_preds.json'\n",
    "sources = []\n",
    "df = pd.read_csv('../assign/examples_to_annotate.csv')\n",
    "for index, row in df.iterrows():\n",
    "    sources.append(row['source'])\n",
    "\n",
    "for idx, gpt_model in enumerate(gpt_models):\n",
    "    print(f\"Run model {idx} - {gpt_model}\")\n",
    "    existing_meta_ids = []\n",
    "    data = {}\n",
    "    if os.path.exists(fname):\n",
    "        with open(fname) as r:\n",
    "            data = json.load(r)\n",
    "            for meta_id in data:\n",
    "                # print(list(data[meta_id].values())[0])\n",
    "                if gpt_model in list(data[meta_id].values())[0]:\n",
    "                    existing_meta_ids.append(meta_id)\n",
    "                \n",
    "    for meta_id in sent_level_labels:\n",
    "        meta_id = str(meta_id)\n",
    "        if meta_id in data:\n",
    "            item = data[meta_id]\n",
    "            print(item)\n",
    "        else:\n",
    "            item = {}\n",
    "        for sent, sent_labels in sent_level_labels[int(meta_id)].items():\n",
    "            if sent not in item:\n",
    "                item[sent] = {'labels': sent_labels}\n",
    "            if meta_id not in existing_meta_ids:\n",
    "                result = call_gpt(system, user.format(article=sources[int(meta_id)], summary=sent), model=gpt_model)\n",
    "                item[sent][gpt_model] = result\n",
    "            \n",
    "        print(item)\n",
    "        if os.path.exists(fname):\n",
    "            with open(fname, 'r') as f:\n",
    "                json_data = json.load(f)\n",
    "                json_data[meta_id] = item\n",
    "        else:\n",
    "            json_data = {meta_id:item}\n",
    "        with open(fname, 'w') as f:\n",
    "            f.write(json.dumps(json_data, indent=2))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
