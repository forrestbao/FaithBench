{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "DOTENV_FILE = \".env\"\n",
    "load_dotenv(DOTENV_FILE, override=True)\n",
    "# caution: path[0] is reserved for script path (or '' in REPL)\n",
    "sys.path.insert(1, '../eval')\n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "label_mapping = {'yes':1, 'no':0, 'accurate':1, 'inaccurate': 0}\n",
    "gpt_models = ['gpt-3.5-turbo', 'gpt-4-turbo', 'gpt-4o', 'gpt-4', 'o1-mini', 'o3-mini']\n",
    "\n",
    "# Aggrefact prompt\n",
    "system = 'Decide if the following summary is consistent with the correponding article. Note that consistency means all information in the summary is supported by the article.'\n",
    "user = 'Article: {article}\\nSummary: {summary}\\nAnswer (yes or no):'\n",
    "\n",
    "def call_gpt(system_prompt, user_prompt, model='gpt-4', temperature=0):\n",
    "    if \"o1\" in model or \"o3\" in model:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": system_prompt + '\\n\\n' + user_prompt}\n",
    "            ],\n",
    "        )\n",
    "        result = completion.choices[0].message.content\n",
    "        result = re.sub(r'[^\\w\\s]', '', result)\n",
    "        result = result.strip().lower().split()\n",
    "        try:\n",
    "            result = label_mapping[result[0]]\n",
    "        except:\n",
    "            if \"accurate\" in result or \"accurately\" in result:\n",
    "                result = 1\n",
    "            elif \"inaccurate\" in result or \"inaccurately\" in result:\n",
    "                result = 0\n",
    "            else:\n",
    "                print(result)\n",
    "                result = None\n",
    "    else:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=temperature\n",
    "        )\n",
    "        result = completion.choices[0].message.content\n",
    "        result = re.sub(r'[^\\w\\s]', '', result)\n",
    "        result = label_mapping[result.strip().lower().split()[0]]\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample-level Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '../assign/examples_to_annotate.csv'\n",
    "# df = pd.read_csv('../assign/examples_to_annotate.csv')\n",
    "# for index, row in df.iterrows():\n",
    "#     sources.append(row['source'])\n",
    "\n",
    "for idx, gpt_model in enumerate(gpt_models):\n",
    "    print(f\"Run model {idx} - {gpt_model}\")\n",
    "    df = pd.read_csv(file_name).fillna('')\n",
    "    if f\"{gpt_model}\" in df:\n",
    "            continue\n",
    "    preds = []\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        # start_time = time.time()\n",
    "        result = call_gpt(system, user.format(article=row['source'], summary=row['summary']), model=gpt_model)\n",
    "        preds.append(result)\n",
    "                    \n",
    "    df.insert(len(df.columns.tolist()), f\"{gpt_model}\", preds)\n",
    "    df.to_csv(file_name, mode='w', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facts_system = '''Your task is to check if the Response is accurate to the Evidence.\n",
    "Generate \\'Accurate\\' if the Response is accurate when verified according to the Evidence,\n",
    "or \\'Inaccurate\\' if the Response is inaccurate (contradicts the evidence) or cannot be\n",
    "verified.'''\n",
    "facts_user = '''**Evidence**\\n\\n{article}\\n\\n**End of Evidence**\\n\n",
    "**Response**:\\n\\n{summary}\\n\\n**End of Response**\\n\n",
    "Let's think step-by-step.'''\n",
    "\n",
    "def load_existing_predictions(filename):\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            return json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        return {}\n",
    "\n",
    "def save_prediction_to_file(filename, data):\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(data, file)\n",
    "\n",
    "for idx, gpt_model in enumerate(gpt_models):\n",
    "    print(f\"Run model {idx} - {gpt_model}\")\n",
    "    df = pd.read_csv(file_name).fillna('')\n",
    "    column_name = f\"{gpt_model}-FACTSprompt\"\n",
    "    if column_name in df:\n",
    "        continue\n",
    "    \n",
    "    # Load or initialize prediction storage\n",
    "    predictions_filename = f\"{gpt_model}_predictions.json\"\n",
    "    predictions = load_existing_predictions(predictions_filename)\n",
    "    print(predictions)\n",
    "    preds = list(predictions.values())\n",
    "    # preds = [predictions.get(str(index), '') for index in df.index]  # Load existing or use empty string\n",
    "    print(len(preds))\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        # start_time = time.time()\n",
    "        # if preds[index]:  # Skip already predicted\n",
    "        #     continue\n",
    "        if index < len(preds):\n",
    "            continue\n",
    "        result = call_gpt(facts_system, facts_user.format(article=row['source'], summary=row['summary']), model=gpt_model)\n",
    "        preds.append(result)\n",
    "        predictions[str(index)] = result\n",
    "        save_prediction_to_file(predictions_filename, predictions)\n",
    "                    \n",
    "    df.insert(len(df.columns.tolist()), f\"{gpt_model}-FACTSprompt\", preds)\n",
    "    df.to_csv(file_name, mode='w', index=False, header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sent-level Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_level_labels = {}\n",
    "result_files, skip_sample_ids, selected_annotators, num_annotators = process_result_files()\n",
    "for file_path in result_files:\n",
    "    _, _, _, batch_sent_level_labels = read_annotation(file_path, skip_sample_ids=skip_sample_ids)\n",
    "    # print(batch_sent_level_labels)\n",
    "    sent_level_labels.update(batch_sent_level_labels)\n",
    "\n",
    "gpt_models = [\"o3-mini\"]\n",
    "\n",
    "fname = '../eval/sent_level_results/detectors_sent_level_preds.json'\n",
    "sources = []\n",
    "df = pd.read_csv('../assign/examples_to_annotate.csv')\n",
    "for index, row in df.iterrows():\n",
    "    sources.append(row['source'])\n",
    "\n",
    "for idx, gpt_model in enumerate(gpt_models):\n",
    "    print(f\"Run model {idx} - {gpt_model}\")\n",
    "    existing_meta_ids = []\n",
    "    data = {}\n",
    "    if os.path.exists(fname):\n",
    "        with open(fname) as r:\n",
    "            data = json.load(r)\n",
    "            for meta_id in data:\n",
    "                # print(list(data[meta_id].values())[0])\n",
    "                if gpt_model in list(data[meta_id].values())[0]:\n",
    "                    existing_meta_ids.append(meta_id)\n",
    "                \n",
    "    for meta_id in tqdm(sent_level_labels):\n",
    "        meta_id = str(meta_id)\n",
    "        if meta_id in data:\n",
    "            item = data[meta_id]\n",
    "            # print(item)\n",
    "        else:\n",
    "            item = {}\n",
    "        for sent, sent_labels in sent_level_labels[int(meta_id)].items():\n",
    "            if sent not in item:\n",
    "                item[sent] = {'labels': sent_labels}\n",
    "            if meta_id not in existing_meta_ids:\n",
    "                result = call_gpt(system, user.format(article=sources[int(meta_id)], summary=sent), model=gpt_model)\n",
    "                item[sent][gpt_model] = result\n",
    "            \n",
    "        # print(item)\n",
    "        if os.path.exists(fname):\n",
    "            with open(fname, 'r') as f:\n",
    "                json_data = json.load(f)\n",
    "                json_data[meta_id] = item\n",
    "        else:\n",
    "            json_data = {meta_id:item}\n",
    "        with open(fname, 'w') as f:\n",
    "            f.write(json.dumps(json_data, indent=2))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faithbench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
