{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Generator, Protocol, List, Tuple\n",
    "import pandas as pd\n",
    "import json\n",
    "from alignscore import AlignScore\n",
    "import os\n",
    "import sys\n",
    "# caution: path[0] is reserved for script path (or '' in REPL)\n",
    "sys.path.insert(1, '../eval')\n",
    "from utils import *\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "ALIGNSCORE_PATH = '/home/miaoran/AlignScore/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AlignScoreEval(model_size, sources, summaries, batch_size=20):\n",
    "    if model_size == 'base':\n",
    "        model = AlignScore(model='roberta-base', batch_size=batch_size, device='cuda:0', ckpt_path=os.path.join(ALIGNSCORE_PATH, 'AlignScore-base.ckpt'), evaluation_mode='bin_sp')\n",
    "    elif model_size == 'large':\n",
    "        model = AlignScore(model='roberta-large', batch_size=batch_size, device='cuda:0', ckpt_path=os.path.join(ALIGNSCORE_PATH, 'AlignScore-large.ckpt'), evaluation_mode='bin_sp')\n",
    "    \n",
    "    scores = model.score(contexts=sources, claims=summaries)\n",
    "    print(scores)    \n",
    "    scores = [round(x,5) for x in scores]\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample-level Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sizes = ['base', 'large']\n",
    "filename = f\"../assign/examples_to_annotate.csv\"\n",
    "outputfile = filename\n",
    "for idx, model_size in enumerate(model_sizes):\n",
    "    print(f\"Run model {str(idx)}: {model_size} ......\")\n",
    "    col_name = f'alignscore-{model_size}'\n",
    "    df = pd.read_csv(filename, encoding='utf-8').fillna('')\n",
    "    scores = AlignScoreEval(model_size, df['source'].tolist(), df['summary'].tolist())\n",
    "    print(scores)\n",
    "    if col_name in df:\n",
    "        df[col_name] = scores\n",
    "    else:\n",
    "        df.insert(len(df.columns), col_name, scores)\n",
    "    df.to_csv(outputfile, mode='w', index=False, header=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sent-level Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_level_labels = {}\n",
    "result_files, skip_sample_ids, selected_annotators, num_annotators = process_result_files()\n",
    "for file_path in result_files:\n",
    "    _, _, _, batch_sent_level_labels = read_annotation(file_path, skip_sample_ids=skip_sample_ids)\n",
    "    # print(batch_sent_level_labels)\n",
    "    sent_level_labels.update(batch_sent_level_labels)\n",
    "# print(sent_level_labels)\n",
    "\n",
    "model_sizes = ['base', 'large']\n",
    "fname = '../eval/sent_level_results/detectors_sent_level_preds.json'\n",
    "sources = []\n",
    "df = pd.read_csv('../assign/examples_to_annotate.csv')\n",
    "for index, row in df.iterrows():\n",
    "    sources.append(row['source'])\n",
    "\n",
    "for idx, model_size in enumerate(model_sizes):\n",
    "    print(f\"Run model {str(idx)}: {model_size} ......\")\n",
    "    existing_meta_ids = []\n",
    "    data = {}\n",
    "    if os.path.exists(fname):\n",
    "        with open(fname) as r:\n",
    "            data = json.load(r)\n",
    "            for meta_id in data:\n",
    "                # print(list(data[meta_id].values())[0])\n",
    "                if f\"alignscore-{model_size}\" in list(data[meta_id].values())[0]:\n",
    "                    existing_meta_ids.append(meta_id)\n",
    "                \n",
    "    for meta_id in sent_level_labels:\n",
    "        meta_id = str(meta_id)\n",
    "        if meta_id in existing_meta_ids:\n",
    "            continue\n",
    "        if meta_id in data:\n",
    "            item = data[meta_id]\n",
    "            # print(item)\n",
    "        else:\n",
    "            item = {}\n",
    "        for sent, sent_labels in sent_level_labels[int(meta_id)].items():\n",
    "            if sent not in item:\n",
    "                item[sent] = {'labels': sent_labels}\n",
    "            item[sent][f\"alignscore-{model_size}\"] = AlignScoreEval(model_size, sources[int(meta_id)], sent)[0]\n",
    "        # print(item)\n",
    "        if os.path.exists(fname):\n",
    "            with open(fname, 'r') as f:\n",
    "                json_data = json.load(f)\n",
    "                json_data[meta_id] = item\n",
    "        else:\n",
    "            json_data = {meta_id:item}\n",
    "        with open(fname, 'w') as f:\n",
    "            f.write(json.dumps(json_data, indent=2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".alignscore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
