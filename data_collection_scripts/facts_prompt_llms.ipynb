{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "DOTENV_FILE = \".env\"\n",
    "load_dotenv(DOTENV_FILE, override=True)\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "# caution: path[0] is reserved for script path (or '' in REPL)\n",
    "sys.path.insert(1, '../eval')\n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "label_mapping = {'accurate':1, 'inaccurate':0}\n",
    "gpt_models = ['gpt-4o', 'gpt-4', 'gpt-4-turbo', 'gpt-3.5-turbo'] #'gpt-3.5-turbo', \n",
    "\n",
    "# Aggrefact prompt\n",
    "system = '''Your task is to check if the Response is accurate to the Evidence.\n",
    "Generate \\'Accurate\\' if the Response is accurate when verified according to the Evidence,\n",
    "or \\'Inaccurate\\' if the Response is inaccurate (contradicts the evidence) or cannot be\n",
    "verified.'''\n",
    "user = '''**Evidence**\\n\\n{article}\\n\\n**End of Evidence**\\n\n",
    "**Response**:\\n\\n{summary}\\n\\n**End of Response**\\n\n",
    "Let's think step-by-step.'''\n",
    "\n",
    "def call_gpt(system_prompt, user_prompt, model='gpt-4', temperature=0):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=temperature\n",
    "    )\n",
    "    result = completion.choices[0].message.content\n",
    "    # print(result)\n",
    "    result = re.sub(r'[^\\w\\s]', '', result)\n",
    "    result = result.strip().lower().split()\n",
    "    try:\n",
    "        result = label_mapping[result[0]]\n",
    "    except:\n",
    "        # print(result)\n",
    "        if \"accurate\" in result or \"accurately\" in result:\n",
    "            result = 1\n",
    "        elif \"inaccurate\" in result or \"inaccurately\" in result:\n",
    "            result = 0\n",
    "        else:\n",
    "            result = None\n",
    "            # raise ValueError\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample-level Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_folder = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(\"__file__\"))), 'backup_data_with_detector_results')\n",
    "# subfolders = [f.path for f in os.scandir(data_folder) if f.is_dir()]\n",
    "# model_files = []\n",
    "# for subfolder in subfolders:\n",
    "#     model_files += [f.path for f in os.scandir(subfolder) if f.is_file()]\n",
    "# print(model_files)\n",
    "# selected_models = [\n",
    "#     \"openai/GPT-3.5-Turbo\",\n",
    "#     \"openai/gpt-4o\",\n",
    "#     \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "#     \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "#     \"cohere/command-r-08-2024\",\n",
    "#     \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "#     \"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
    "#     \"google/gemini-1.5-flash-001\",\n",
    "#     \"Anthropic/claude-3-5-sonnet-20240620\",\n",
    "#     \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "# ]\n",
    "\n",
    "# for idx, gpt_model in enumerate(gpt_models):\n",
    "#     print(f\"Run model {idx} - {gpt_model}\")\n",
    "#     for file_name in model_files:\n",
    "#         print(file_name.replace(data_folder,'')[1:].replace('.csv',''))\n",
    "#         if file_name.replace(data_folder,'')[1:].replace('.csv','') not in selected_models:\n",
    "#             continue\n",
    "#         with open(file_name) as f:\n",
    "#             print(file_name)\n",
    "#             df = pd.read_csv(file_name).fillna('')\n",
    "#             if f\"{gpt_model}-FACTSprompt\" in df:\n",
    "#                 continue\n",
    "#             preds = []\n",
    "#             for index, row in df.iterrows():\n",
    "#                 # start_time = time.time()\n",
    "#                 result = call_gpt(system, user.format(article=row['source'], summary=row['summary']), model=gpt_model)\n",
    "#                 preds.append(result)\n",
    "                    \n",
    "#             df.insert(len(df.columns.tolist()), f\"{gpt_model}-FACTSprompt\", preds)\n",
    "#             df.to_csv(file_name, mode='w', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run model 0 - gpt-4o\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/1150 [00:14<49:50,  2.61s/it]  "
     ]
    }
   ],
   "source": [
    "file_name = '../assign/examples_to_annotate.csv'\n",
    "# df = pd.read_csv('../assign/examples_to_annotate.csv')\n",
    "# for index, row in df.iterrows():\n",
    "#     sources.append(row['source'])\n",
    "\n",
    "for idx, gpt_model in enumerate(gpt_models):\n",
    "    print(f\"Run model {idx} - {gpt_model}\")\n",
    "    df = pd.read_csv(file_name).fillna('')\n",
    "    if f\"{gpt_model}-FACTSprompt\" in df:\n",
    "            continue\n",
    "    preds = []\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        # start_time = time.time()\n",
    "        result = call_gpt(system, user.format(article=row['source'], summary=row['summary']), model=gpt_model)\n",
    "        preds.append(result)\n",
    "                    \n",
    "    df.insert(len(df.columns.tolist()), f\"{gpt_model}-FACTSprompt\", preds)\n",
    "    df.to_csv(file_name, mode='w', index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faithbench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
