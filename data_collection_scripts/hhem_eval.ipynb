{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miaoran/.env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification, AutoModelForTokenClassification\n",
    "from peft import PeftModel\n",
    "from datasets import load_dataset\n",
    "from typing import Any, Generator, Protocol, List, Tuple\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "if torch.cuda.is_available():\n",
    "  dev = \"cuda:0\"\n",
    "else:\n",
    "  dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_batch(source: List[str], summary: List[str], batch_size: int) -> Generator:\n",
    "    l = len(source)\n",
    "    for ndx in range(0, l, batch_size):\n",
    "        batch = []\n",
    "        for i in range(ndx, min(ndx + batch_size, l)):\n",
    "            batch.append([source[i], summary[i]])\n",
    "        \n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HHEMv1Eval(filename, col_name = 'HHEMv1', device=device, update=True, batch_size=20):\n",
    "    df = pd.read_csv(filename, encoding='utf-8').fillna('')\n",
    "    if (not update) and (col_name in df): # store HHEM scores\n",
    "        return\n",
    "    model = AutoModelForSequenceClassification.from_pretrained('vectara/hallucination_evaluation_model',revision = 'hhem-1.0-open').to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained('vectara/hallucination_evaluation_model', revision = 'hhem-1.0-open')\n",
    "    scores = []\n",
    "    for batch in _create_batch(df['source'].tolist(), df['summary'].tolist(), batch_size):\n",
    "        inputs = tokenizer.batch_encode_plus(batch, return_tensors='pt', padding=True).to(device)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits.cpu().numpy()\n",
    "            # convert logits to probabilities\n",
    "            batch_scores = 1 / (1 + np.exp(-logits)).flatten()\n",
    "            batch_scores = [round(x, 5) for x in batch_scores]\n",
    "            scores += batch_scores\n",
    "    # print(scores)\n",
    "    if col_name in df:\n",
    "        df[col_name] = scores\n",
    "    else:\n",
    "        df.insert(len(df.columns), col_name, scores)\n",
    "    df.to_csv(filename, mode='w', index=False, header=True)\n",
    "    print('HHEMv1 Scores have been saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HHEM21Eval(filename, col_name = 'HHEM-2.1', device=device, update=True, batch_size=20):\n",
    "    df = pd.read_csv(filename, encoding='utf-8').fillna('')\n",
    "    if (not update) and (col_name in df): # store HHEM scores\n",
    "        return\n",
    "    prompt = \"<pad> Determine if the hypothesis is true given the premise?\\n\\nPremise: {text1}\\n\\nHypothesis: {text2}\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained('t5-base')\n",
    "    config = AutoConfig.from_pretrained('google/flan-t5-large')\n",
    "    model = AutoModelForTokenClassification.from_pretrained('vectara/HHEM-2.1', config=config).to(device)\n",
    "    scores = []\n",
    "    for batch in _create_batch(df['source'].tolist(), df['summary'].tolist(), batch_size):\n",
    "        inputs = tokenizer([prompt.format(text1=pair[0], text2=pair[1]) for pair in batch], \n",
    "                            return_tensors='pt', padding='longest').to(device)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(**inputs)\n",
    "        logits = output.logits\n",
    "\n",
    "        logits = logits[:,0,:] # get the logits on the first token\n",
    "\n",
    "        logits = torch.softmax(logits, dim=-1)\n",
    "        batch_scores = [round(x, 5) for x in logits[:, 1].tolist()] # list of float\n",
    "        scores += batch_scores\n",
    "    # print(scores)\n",
    "    print(len(scores))\n",
    "    if col_name in df:\n",
    "        df[col_name] = scores\n",
    "    else:\n",
    "        df.insert(len(df.columns), col_name, scores)\n",
    "    df.to_csv(filename, mode='w', index=False, header=True)\n",
    "    print('HHEM-2.1 Scores have been saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HHEM21EnglishEval(filename, col_name = 'HHEM-2.1-English', device=device, update=True, batch_size=20):\n",
    "    df = pd.read_csv(filename, encoding='utf-8').fillna('')\n",
    "    if (not update) and (col_name in df): # store HHEM scores\n",
    "        return\n",
    "    prompt = \"<pad> Determine if the hypothesis is true given the premise?\\n\\nPremise: {text1}\\n\\nHypothesis: {text2}\"\n",
    "    config = AutoConfig.from_pretrained('google/flan-t5-large')\n",
    "    tokenizer = AutoTokenizer.from_pretrained('t5-base')\n",
    "    model = AutoModelForTokenClassification.from_pretrained('vectara/HHEM-2.1-English', config=config).to(device)\n",
    "    scores = []\n",
    "    for batch in _create_batch(df['source'].tolist(), df['summary'].tolist(), batch_size):\n",
    "        inputs = tokenizer([prompt.format(text1=pair[0], text2=pair[1]) for pair in batch], \n",
    "                            return_tensors='pt', padding='longest').to(device)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(**inputs)\n",
    "        logits = output.logits\n",
    "\n",
    "        logits = logits[:,0,:] # get the logits on the first token\n",
    "\n",
    "        logits = torch.softmax(logits, dim=-1)\n",
    "        batch_scores = [round(x, 5) for x in logits[:, 1].tolist()] # list of float\n",
    "        scores += batch_scores\n",
    "    # print(scores)\n",
    "    if col_name in df:\n",
    "        df[col_name] = scores\n",
    "    else:\n",
    "        df.insert(len(df.columns), col_name, scores)\n",
    "    df.to_csv(filename, mode='w', index=False, header=True)\n",
    "    print('HHEM-2.1-English Scores have been saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'anthropic/Claude-3-sonnet', 'google/gemma-7b-it', 'mistralai/Mixtral-8x7B-Instruct-v0.1', 'google/gemma-1.1-7b-it', 'meta-llama/Llama-3-70B-chat-hf', 'microsoft/Phi-3.5-MoE-instruct', 'microsoft/WizardLM-2-8x22B', 'microsoft/Phi-2', 'databricks/dbrx-instruct', 'google/gemma-2-9b-it', 'Anthropic/claude-3-5-sonnet-20240620', 'openai/GPT-4o-mini', 'google/PaLM-2', 'meta-llama/Llama-3-8B-chat-hf', 'deepseek/deepseek-chat', 'google/gemini-1.5-pro-001', 'meta-llama/Llama-2-13b-chat-hf', 'amazon/Titan-Express', '01-ai/Yi-1.5-34B-Chat', 'openai/o1-preview', 'cohere/command-r-plus-08-2024', '01-ai/Yi-1.5-9B-Chat', 'google/gemma-1.1-2b-it', 'meta-llama/Meta-Llama-3.1-70B-Instruct', 'anthropic/Claude-2', 'openai/GPT-3.5-Turbo', 'openai/gpt-4o', 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'microsoft/Phi-3-mini-128k-instruct', 'cohere/Cohere', 'google/Gemini-1.5-Pro', 'mistralai/Mistral-7B-Instruct-v0.3', 'Qwen/Qwen2-VL-2B-Instruct', 'google/Gemini-1.5-flash', 'google/gemini-pro-experimental', 'mistralai/Mistral-Nemo-Instruct-2407', 'ai21labs/AI21-Jamba-1.5-Mini', 'openai/o1-mini', 'meta-llama/Llama-2-7b-chat-hf', 'microsoft/Orca-2-13b', 'mistralai/Mistral-Large2', 'THUDM/glm-4-9b-chat', 'google/gemini-flash-experimental', 'openai/GPT-4-Turbo-2024-04-09', 'mistralai/Mixtral-8x22B-Instruct-v0.1', 'apple/OpenELM-3B-Instruct', 'meta-llama/Llama-2-70b-chat-hf', 'google/PaLM-2-Chat', 'anthropic/Claude-3-opus', 'Qwen/Qwen2-72B-Instruct', 'meta-llama/Meta-Llama-3.1-405B-Instruct', 'google/Gemini-Pro', 'google/flan-t5-large', 'google/gemini-1.5-flash-001', 'microsoft/Phi-3-mini-4k-instruct', 'microsoft/Phi-3.5-mini-instruct', 'tiiuae/falcon-7b-instruct', 'openai/GPT-4', 'CohereForAI/c4ai-command-r-plus', '01-ai/Yi-1.5-6B-Chat', 'cohere/Cohere-Chat', 'google/gemma-2-2b-it', 'snowflake/snowflake-arctic-instruct', 'Intel/neural-chat-7b-v3-3', 'cohere/command-r-08-2024', 'Qwen/Qwen2-VL-7B-Instruct'}\n",
      "66\n",
      "Processing file 9: google/gemma-2-9b-it.csv ......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (813 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HHEMv1 Scores have been saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miaoran/.env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of T5ForTokenClassification were not initialized from the model checkpoint at google/flan-t5-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1006\n",
      "HHEM-2.1 Scores have been saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miaoran/.env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of T5ForTokenClassification were not initialized from the model checkpoint at google/flan-t5-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HHEM-2.1-English Scores have been saved\n",
      "Finshed google/gemma-2-9b-it.csv\n",
      "====================\n",
      "Processing file 21: 01-ai/Yi-1.5-9B-Chat.csv ......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (778 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HHEMv1 Scores have been saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miaoran/.env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of T5ForTokenClassification were not initialized from the model checkpoint at google/flan-t5-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1006\n",
      "HHEM-2.1 Scores have been saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miaoran/.env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of T5ForTokenClassification were not initialized from the model checkpoint at google/flan-t5-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HHEM-2.1-English Scores have been saved\n",
      "Finshed 01-ai/Yi-1.5-9B-Chat.csv\n",
      "====================\n",
      "Processing file 24: anthropic/Claude-2.csv ......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1049 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HHEMv1 Scores have been saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miaoran/.env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of T5ForTokenClassification were not initialized from the model checkpoint at google/flan-t5-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "831\n",
      "HHEM-2.1 Scores have been saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miaoran/.env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of T5ForTokenClassification were not initialized from the model checkpoint at google/flan-t5-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HHEM-2.1-English Scores have been saved\n",
      "Finshed anthropic/Claude-2.csv\n",
      "====================\n",
      "Processing file 27: meta-llama/Meta-Llama-3.1-8B-Instruct.csv ......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (770 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HHEMv1 Scores have been saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miaoran/.env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of T5ForTokenClassification were not initialized from the model checkpoint at google/flan-t5-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1006\n",
      "HHEM-2.1 Scores have been saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miaoran/.env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of T5ForTokenClassification were not initialized from the model checkpoint at google/flan-t5-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HHEM-2.1-English Scores have been saved\n",
      "Finshed meta-llama/Meta-Llama-3.1-8B-Instruct.csv\n",
      "====================\n",
      "Processing file 37: openai/o1-mini.csv ......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (785 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HHEMv1 Scores have been saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miaoran/.env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of T5ForTokenClassification were not initialized from the model checkpoint at google/flan-t5-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1006\n",
      "HHEM-2.1 Scores have been saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miaoran/.env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of T5ForTokenClassification were not initialized from the model checkpoint at google/flan-t5-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HHEM-2.1-English Scores have been saved\n",
      "Finshed openai/o1-mini.csv\n",
      "====================\n",
      "Processing file 51: google/Gemini-Pro.csv ......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (852 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HHEMv1 Scores have been saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miaoran/.env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of T5ForTokenClassification were not initialized from the model checkpoint at google/flan-t5-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1006\n",
      "HHEM-2.1 Scores have been saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miaoran/.env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of T5ForTokenClassification were not initialized from the model checkpoint at google/flan-t5-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HHEM-2.1-English Scores have been saved\n",
      "Finshed google/Gemini-Pro.csv\n",
      "====================\n",
      "Processing file 58: CohereForAI/c4ai-command-r-plus.csv ......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (768 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HHEMv1 Scores have been saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miaoran/.env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of T5ForTokenClassification were not initialized from the model checkpoint at google/flan-t5-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1006\n",
      "HHEM-2.1 Scores have been saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miaoran/.env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of T5ForTokenClassification were not initialized from the model checkpoint at google/flan-t5-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HHEM-2.1-English Scores have been saved\n",
      "Finshed CohereForAI/c4ai-command-r-plus.csv\n",
      "====================\n",
      "Processing file 59: 01-ai/Yi-1.5-6B-Chat.csv ......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (866 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HHEMv1 Scores have been saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miaoran/.env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of T5ForTokenClassification were not initialized from the model checkpoint at google/flan-t5-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1006\n",
      "HHEM-2.1 Scores have been saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miaoran/.env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of T5ForTokenClassification were not initialized from the model checkpoint at google/flan-t5-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HHEM-2.1-English Scores have been saved\n",
      "Finshed 01-ai/Yi-1.5-6B-Chat.csv\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "complete_df = pd.read_csv('../leaderboard_results/leaderboard_summaries.csv', encoding='utf-8')\n",
    "models = set(complete_df['model'].values.tolist())\n",
    "print(models)\n",
    "print(len(models))\n",
    "for idx, model_name in enumerate(models):\n",
    "    if model_name not in ['openai/o1-mini', 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'google/Gemini-Pro','google/gemma-2-9b-it','CohereForAI/c4ai-command-r-plus','anthropic/Claude-2', '01-ai/Yi-1.5-6B-Chat', '01-ai/Yi-1.5-9B-Chat']:\n",
    "        continue\n",
    "    filename = model_name + '.csv'\n",
    "    print(f\"Processing file {str(idx)}: {filename} ......\")\n",
    "    HHEMv1Eval(filename, batch_size=15)\n",
    "    HHEM21Eval(filename, batch_size=15)\n",
    "    HHEM21EnglishEval(filename, batch_size=15)\n",
    "    print(f\"Finshed {filename}\")\n",
    "    print('='*20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
